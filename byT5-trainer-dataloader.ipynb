{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8436a50",
   "metadata": {},
   "source": [
    "# Training a model with custom DataLoader\n",
    "meant to handle problems with placing generator on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebe0dee-1ee6-41c8-9ebb-5d64ea9f93cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mapto/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# from glob import glob\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# import wandb\n",
    "from datasets import Dataset as HFDataset\n",
    "from evaluate import load\n",
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "from transformers import ByT5Tokenizer  # a \"dummy\" tokenizer, tokenizing into bytes\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "from config import data_root, model_root, checkpoint_name\n",
    "from config import token_len, annot_len\n",
    "from config import device as device_choice, batch_size, token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcf1d65-b9c0-4d65-acd4-de483927446e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/accelerate/main/en/package_reference/utilities#accelerate.DataLoaderConfiguration\n",
    "dataloader_config = DataLoaderConfiguration(\n",
    "    use_seedable_sampler=False,\n",
    ")\n",
    "# https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator#accelerate.Accelerator\n",
    "accelerator = Accelerator(\n",
    "    dataloader_config=dataloader_config,\n",
    "    project_dir=model_root,\n",
    "    # rng_types=\"torch\",\n",
    "    # rng_types=\"cuda\",\n",
    "    # rng_types=\"generator\",\n",
    "    cpu=device_choice == 'cpu',\n",
    ")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "device = accelerator.device\n",
    "torch.set_default_device(device)\n",
    "# torch.cuda.is_available()\n",
    "# accelerator.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f4a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "# for fname in glob(f\"{data_root}/*.csv\"):\n",
    "#     dfs += [pd.read_csv(fname, names=[\"inputs\", \"labels\"])]\n",
    "with zipfile.ZipFile(f\"{data_root}/data-ue.zip\") as zf:\n",
    "    for name in zf.namelist():\n",
    "        dfs += [pd.read_csv(zf.open(name), names=[\"input\", \"label\"])]\n",
    "df = pd.concat(dfs, axis=0)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3368dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                     | 0/41104 [00:00<?, ? examples/s]/home/mapto/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4144: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████| 41104/41104 [00:01<00:00, 22867.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 41104\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/model_doc/byt5#transformers.ByT5Tokenizer\n",
    "tokenizer = ByT5Tokenizer()\n",
    "\n",
    "\n",
    "# Function to tokenize data\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"], max_length=token_len, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"], max_length=token_len, truncation=True, padding=\"max_length\"\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = [\n",
    "        -100 if token == tokenizer.pad_token_id else token\n",
    "        for token in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "data = {\"text\": df[\"input\"].to_list(), \"target\": df[\"label\"].to_list()}\n",
    "hf_dataset = HFDataset.from_dict(data)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = hf_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"target\"]\n",
    ")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af58909e-64d4-4458-885a-22901de06e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 41104\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom DataLoader\n",
    "class HFDatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "wrapped_dataset = HFDatasetWrapper(tokenized_dataset)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 2\n",
    "dataloader = DataLoader(\n",
    "    wrapped_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24eb82a7-efda-4e01-952e-2f5860603892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/spaces/evaluate-metric/exact_match\n",
    "exact_match_metric = load(\"exact_match\")\n",
    "\n",
    "\n",
    "def compute_exact_match(pred: EvalPrediction):\n",
    "    # Convert predictions to text\n",
    "    predictions = pred.predictions\n",
    "    references = pred.label_ids\n",
    "\n",
    "    # Decode if needed\n",
    "    decoded_preds = [\n",
    "        pred.decode(pred, skip_special_tokens=True) for pred in predictions\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        label.decode(label, skip_special_tokens=True) for label in references\n",
    "    ]\n",
    "\n",
    "    # Compute exact match\n",
    "    result = exact_match_metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels\n",
    "    )\n",
    "    return {\"exact_match\": result[\"exact_match\"]}\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_root}/byT5-ocs-ue\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    #use_cache=False,\n",
    "    torch_empty_cache_steps=100,\n",
    "    disable_tqdm=False,\n",
    "    report_to=None,  # disable wandb.ai\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    use_cpu=device_choice=='cpu',\n",
    ")\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration\n",
    "def init_model():\n",
    "    model = T5ForConditionalGeneration(config)\n",
    "    # model = model.cuda()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Config\n",
    "config = T5Config.from_pretrained(\"t5-base\")\n",
    "# config.task_specific_params = {}\n",
    "# https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = batch_size\n",
    "dataloader = DataLoader(\n",
    "    wrapped_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c98697-ef34-49f7-bcd0-384ede558491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model_init=init_model,\n",
    "    args=args,\n",
    "    train_dataset=wrapped_dataset,\n",
    "    eval_dataset=wrapped_dataset,  # TODO\n",
    "    # train_dataset=wrapped_dataset[\"train\"],\n",
    "    # eval_dataset=wrapped_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_exact_match,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd1633-8da7-482c-9200-640a1d45b6bf",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mapto/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/mapto/.local/share/pipx/venvs/jupyterlab/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   15/15414 00:28 < 9:16:54, 0.46 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fd845",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir=f\"{model_root}/byT5-ocs-ue-final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
