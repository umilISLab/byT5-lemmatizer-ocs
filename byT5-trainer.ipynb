{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8436a50",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "see README.md for details\n",
    "\n",
    "Loosely following this tutorial:\n",
    "https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebe0dee-1ee6-41c8-9ebb-5d64ea9f93cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:12:55.033646Z",
     "iopub.status.busy": "2024-07-12T22:12:55.032922Z",
     "iopub.status.idle": "2024-07-12T22:12:57.009250Z",
     "shell.execute_reply": "2024-07-12T22:12:57.008591Z",
     "shell.execute_reply.started": "2024-07-12T22:12:55.033576Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from config import data_root, model_root, checkpoint_name\n",
    "from config import token_len, annot_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcf1d65-b9c0-4d65-acd4-de483927446e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:12:57.122184Z",
     "iopub.status.busy": "2024-07-12T22:12:57.122014Z",
     "iopub.status.idle": "2024-07-12T22:12:57.163190Z",
     "shell.execute_reply": "2024-07-12T22:12:57.162612Z",
     "shell.execute_reply.started": "2024-07-12T22:12:57.122169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a5d8b9-3870-4c7f-aaa5-22996332d444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:12:57.164982Z",
     "iopub.status.busy": "2024-07-12T22:12:57.164531Z",
     "iopub.status.idle": "2024-07-12T22:13:01.725808Z",
     "shell.execute_reply": "2024-07-12T22:13:01.725310Z",
     "shell.execute_reply.started": "2024-07-12T22:12:57.164964Z"
    }
   },
   "outputs": [],
   "source": [
    "num_special_tokens = 3\n",
    "model_checkpoint = \"google/byt5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449ddcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_pandas in module datasets.arrow_dataset:\n",
      "\n",
      "from_pandas(df: pandas.core.frame.DataFrame, features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, preserve_index: Optional[bool] = None) -> 'Dataset' method of builtins.type instance\n",
      "    Convert `pandas.DataFrame` to a `pyarrow.Table` to create a [`Dataset`].\n",
      "    \n",
      "    The column types in the resulting Arrow Table are inferred from the dtypes of the `pandas.Series` in the\n",
      "    DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the\n",
      "    case of `object`, we need to guess the datatype by looking at the Python objects in this Series.\n",
      "    \n",
      "    Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow\n",
      "    type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only\n",
      "    contains `None/nan` objects, the type is set to `null`. This behavior can be avoided by constructing explicit\n",
      "    features and passing it to this function.\n",
      "    \n",
      "    Args:\n",
      "        df (`pandas.DataFrame`):\n",
      "            Dataframe that contains the dataset.\n",
      "        features ([`Features`], *optional*):\n",
      "            Dataset features.\n",
      "        info (`DatasetInfo`, *optional*):\n",
      "            Dataset information, like description, citation, etc.\n",
      "        split (`NamedSplit`, *optional*):\n",
      "            Name of the dataset split.\n",
      "        preserve_index (`bool`, *optional*):\n",
      "            Whether to store the index as an additional column in the resulting Dataset.\n",
      "            The default of `None` will store the index as a column, except for `RangeIndex` which is stored as metadata only.\n",
      "            Use `preserve_index=True` to force it to be stored as a column.\n",
      "    \n",
      "    Returns:\n",
      "        [`Dataset`]\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```py\n",
      "    >>> ds = Dataset.from_pandas(df)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Dataset.from_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d52fc11b-6eeb-45e2-a4c2-753025389299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T22:13:01.727005Z",
     "iopub.status.busy": "2024-07-12T22:13:01.726446Z",
     "iopub.status.idle": "2024-07-12T22:13:01.737390Z",
     "shell.execute_reply": "2024-07-12T22:13:01.737014Z",
     "shell.execute_reply.started": "2024-07-12T22:13:01.726985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Value(dtype='string', id=None),\n",
       " 'test': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for fname in glob(f\"{data_root}/*.csv\"):\n",
    "    dfs += [pd.read_csv(fname, names=[\"train\",\"test\"])]\n",
    "df = pd.concat(dfs, axis=0)\n",
    "ds = Dataset.from_pandas(df)\n",
    "# ds = load_dataset(\"csv\", data_files=glob(f\"{data_root}/*.csv\"), features=[\"input\",\"output\"])\n",
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff2a0587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497e967c6fca41eba8f74b0984ef4f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'torch_empty_cache_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m exact_match_metric \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact_match\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_empty_cache_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_init\u001b[39m():\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint)\n",
      "\u001b[0;31mTypeError\u001b[0m: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'torch_empty_cache_steps'"
     ]
    }
   ],
   "source": [
    "exact_match_metric = load(\"exact_match\")\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    torch_empty_cache_steps=100,\n",
    ")\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    #     data_collator=data_collator,\n",
    "    compute_metrics=exact_match_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
