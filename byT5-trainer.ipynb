{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8436a50",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "see README.md for details\n",
    "\n",
    "Loosely following this tutorial:\n",
    "https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebe0dee-1ee6-41c8-9ebb-5d64ea9f93cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:23.168258Z",
     "iopub.status.busy": "2024-07-15T15:55:23.167805Z",
     "iopub.status.idle": "2024-07-15T15:55:30.593123Z",
     "shell.execute_reply": "2024-07-15T15:55:30.592542Z",
     "shell.execute_reply.started": "2024-07-15T15:55:23.168239Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import pandas as pd\n",
    "# from glob import glob\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import T5Config\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import ByT5Tokenizer  # a \"dummy\" tokenizer, tokenizing into bytes\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from config import data_root, model_root, checkpoint_name\n",
    "from config import token_len, annot_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf1d65-b9c0-4d65-acd4-de483927446e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:30.594569Z",
     "iopub.status.busy": "2024-07-15T15:55:30.594150Z",
     "iopub.status.idle": "2024-07-15T15:55:30.599692Z",
     "shell.execute_reply": "2024-07-15T15:55:30.599171Z",
     "shell.execute_reply.started": "2024-07-15T15:55:30.594525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f4a3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:35.835841Z",
     "iopub.status.busy": "2024-07-15T15:55:35.835062Z",
     "iopub.status.idle": "2024-07-15T15:55:35.855998Z",
     "shell.execute_reply": "2024-07-15T15:55:35.855551Z",
     "shell.execute_reply.started": "2024-07-15T15:55:35.835812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'labels', '__index_level_0__'],\n",
       "        num_rows: 36995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'labels', '__index_level_0__'],\n",
       "        num_rows: 4111\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "# for fname in glob(f\"{data_root}/*.csv\"):\n",
    "#     dfs += [pd.read_csv(fname, names=[\"inputs\", \"labels\"])]\n",
    "with zipfile.ZipFile(f\"{data_root}/data.zip\") as zf:\n",
    "    for name in zf.namelist():\n",
    "        dfs += [pd.read_csv(zf.open(name), names=[\"inputs\", \"labels\"])]\n",
    "df = pd.concat(dfs, axis=0)\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3368dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByT5Tokenizer()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "\n",
    "# Create a tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"inputs\"]\n",
    "    targets = examples[\"labels\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=64, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "tds = ds.map(preprocess_function, batched=True)\n",
    "tds = tds.remove_columns([\"inputs\"])\n",
    "tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb82a7-efda-4e01-952e-2f5860603892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:39.701958Z",
     "iopub.status.busy": "2024-07-15T15:55:39.701458Z",
     "iopub.status.idle": "2024-07-15T15:55:43.130357Z",
     "shell.execute_reply": "2024-07-15T15:55:43.129764Z",
     "shell.execute_reply.started": "2024-07-15T15:55:39.701936Z"
    }
   },
   "outputs": [],
   "source": [
    "exact_match_metric = load(\"exact_match\")\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_root}/byT5-ocs-ue\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    #     gradient_checkpointing=True,\n",
    "    # torch_empty_cache_steps=100,\n",
    "    disable_tqdm=False,\n",
    "    report_to=None,  # disable wandb.ai\n",
    ")\n",
    "\n",
    "config = T5Config.from_pretrained(\"t5-base\")\n",
    "# config.task_specific_params = {}\n",
    "model = T5ForConditionalGeneration(config)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c98697-ef34-49f7-bcd0-384ede558491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:56:10.167944Z",
     "iopub.status.busy": "2024-07-15T15:56:10.167719Z",
     "iopub.status.idle": "2024-07-15T15:56:11.815154Z",
     "shell.execute_reply": "2024-07-15T15:56:11.814603Z",
     "shell.execute_reply.started": "2024-07-15T15:56:10.167927Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tds[\"train\"],\n",
    "    eval_dataset=tds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=exact_match_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd1633-8da7-482c-9200-640a1d45b6bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:56:17.372745Z",
     "iopub.status.busy": "2024-07-15T15:56:17.372507Z",
     "iopub.status.idle": "2024-07-15T15:56:18.125324Z",
     "shell.execute_reply": "2024-07-15T15:56:18.124543Z",
     "shell.execute_reply.started": "2024-07-15T15:56:17.372720Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fd845",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
