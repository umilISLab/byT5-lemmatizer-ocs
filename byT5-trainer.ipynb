{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8436a50",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "see README.md for details\n",
    "\n",
    "Loosely following this tutorial:\n",
    "https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebe0dee-1ee6-41c8-9ebb-5d64ea9f93cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:23.168258Z",
     "iopub.status.busy": "2024-07-15T15:55:23.167805Z",
     "iopub.status.idle": "2024-07-15T15:55:30.593123Z",
     "shell.execute_reply": "2024-07-15T15:55:30.592542Z",
     "shell.execute_reply.started": "2024-07-15T15:55:23.168239Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset, Features, load_dataset\n",
    "from evaluate import load\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import T5Config, T5Model\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import ByT5Tokenizer  # a \"dummy\" tokenizer, tokenizing into bytes\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from config import data_root, model_root, checkpoint_name\n",
    "from config import token_len, annot_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcf1d65-b9c0-4d65-acd4-de483927446e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:30.594569Z",
     "iopub.status.busy": "2024-07-15T15:55:30.594150Z",
     "iopub.status.idle": "2024-07-15T15:55:30.599692Z",
     "shell.execute_reply": "2024-07-15T15:55:30.599171Z",
     "shell.execute_reply.started": "2024-07-15T15:55:30.594525Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f4a3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:35.835841Z",
     "iopub.status.busy": "2024-07-15T15:55:35.835062Z",
     "iopub.status.idle": "2024-07-15T15:55:35.855998Z",
     "shell.execute_reply": "2024-07-15T15:55:35.855551Z",
     "shell.execute_reply.started": "2024-07-15T15:55:35.835812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'labels', '__index_level_0__'],\n",
       "        num_rows: 36993\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'labels', '__index_level_0__'],\n",
       "        num_rows: 4111\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for fname in glob(f\"{data_root}/*.csv\"):\n",
    "    dfs += [pd.read_csv(fname, names=[\"inputs\", \"labels\"])]\n",
    "df = pd.concat(dfs, axis=0)\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3368dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21b22d1c5564c33824c531965561396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a552990895ef4b96ae90acf7e8eb7ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'labels', '__index_level_0__', 'input_ids', 'attention_mask', 'decoder_input_ids'],\n",
       "        num_rows: 36993\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'labels', '__index_level_0__', 'input_ids', 'attention_mask', 'decoder_input_ids'],\n",
       "        num_rows: 4111\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = ByT5Tokenizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "\n",
    "# Create a tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"inputs\"]\n",
    "    targets = examples[\"labels\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=64, truncation=True)\n",
    "\n",
    "    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "tds = ds.map(preprocess_function, batched=True)\n",
    "# tds = tds.remove_columns([\"input\", \"label\"])\n",
    "tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24eb82a7-efda-4e01-952e-2f5860603892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:55:39.701958Z",
     "iopub.status.busy": "2024-07-15T15:55:39.701458Z",
     "iopub.status.idle": "2024-07-15T15:55:43.130357Z",
     "shell.execute_reply": "2024-07-15T15:55:43.129764Z",
     "shell.execute_reply.started": "2024-07-15T15:55:39.701936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "exact_match_metric = load(\"exact_match\")\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_root}/byT5-ocs-ue\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    # torch_empty_cache_steps=100,\n",
    "    disable_tqdm=False,\n",
    "    report_to=None,  # disable wandb.ai\n",
    ")\n",
    "\n",
    "config = T5Config.from_pretrained(\"t5-base\")\n",
    "# config.task_specific_params = {}\n",
    "model = T5Model(config)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c98697-ef34-49f7-bcd0-384ede558491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:56:10.167944Z",
     "iopub.status.busy": "2024-07-15T15:56:10.167719Z",
     "iopub.status.idle": "2024-07-15T15:56:11.815154Z",
     "shell.execute_reply": "2024-07-15T15:56:11.814603Z",
     "shell.execute_reply.started": "2024-07-15T15:56:10.167927Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tds[\"train\"],\n",
    "    eval_dataset=tds[\"test\"],\n",
    "    # train_dataset=small_train_dataset,\n",
    "    # eval_dataset=small_eval_dataset,\n",
    "    #     data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=exact_match_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53cd1633-8da7-482c-9200-640a1d45b6bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T15:56:17.372745Z",
     "iopub.status.busy": "2024-07-15T15:56:17.372507Z",
     "iopub.status.idle": "2024-07-15T15:56:18.125324Z",
     "shell.execute_reply": "2024-07-15T15:56:18.124543Z",
     "shell.execute_reply.started": "2024-07-15T15:56:17.372720Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5Model.forward` and have been ignored: labels, __index_level_0__, inputs.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 36993\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 27744\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1365\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1365\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1368\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1371\u001b[0m ):\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1940\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1943\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1982\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1979\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoother(outputs, labels)\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[0;32m-> 1982\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/file_utils.py:2597\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2596\u001b[0m     inner_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 2597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tuple()[k]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fd845",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
